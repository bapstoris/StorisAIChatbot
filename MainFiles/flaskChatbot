from flask import Flask, render_template, request, jsonify
from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

app = Flask(__name__)

# --- Build / load your vector store once at startup ---
with open("combined_pdf_text.txt", "r", encoding="utf-8") as f:
    all_text = f.read()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = splitter.split_text(all_text)

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(docs, embeddings)
# Optionally: vectorstore = FAISS.load_local("faiss_index_openai", embeddings)

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0) #change to whichever model is needed
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
)

@app.route("/")
def index():
    return render_template("index.html")  # your chatbot UI

@app.route("/chatbot", methods=["POST"])
def chatbot():
    data = request.get_json()
    question = data.get("question", "")
    answer = qa_chain.run(question)
    return jsonify({"response": answer})

if __name__ == "__main__":
    app.run(debug=True)